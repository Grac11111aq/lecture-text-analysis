#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰å“è³ªæ”¹å–„ã®ãŸã‚ã®èªå½™åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ•™è‚²ãƒ‡ãƒ¼ã‚¿ã®èªå½™ã‚’åˆ†é¡ãƒ»æ­£è¦åŒ–ã—ã¦è³ªã®é«˜ã„ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
"""

import pandas as pd
import re
from collections import Counter, defaultdict
from janome.tokenizer import Tokenizer
import json

def load_and_analyze_data():
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸºæœ¬çµ±è¨ˆ"""
    df = pd.read_csv('data/processed/all_text_corpus.csv')
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦: {len(df)}ä»¶")
    print(f"ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ: {df['category'].value_counts().to_dict()}")
    print(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {df['class'].value_counts().to_dict()}")
    return df

def tokenize_and_analyze(df):
    """å½¢æ…‹ç´ è§£æã¨èªå½™åˆ†æ"""
    tokenizer = Tokenizer()
    
    all_words = []
    word_categories = defaultdict(list)
    word_contexts = defaultdict(list)
    
    for idx, row in df.iterrows():
        text = str(row['text'])
        category = row['category']
        
        # å½¢æ…‹ç´ è§£æ
        tokens = tokenizer.tokenize(text)
        for token in tokens:
            try:
                word = token.surface
                # features ãŒ None ã®å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                if hasattr(token, 'features') and token.features:
                    pos = token.features.split(',')[0]  # å“è©
                else:
                    pos = 'Unknown'
                
                # åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if len(word) >= 1 and word not in ['ã€', 'ã€‚', 'ï¼Ÿ', 'ï¼']:
                    all_words.append(word)
                    word_categories[category].append(word)
                    word_contexts[word].append({
                        'category': category,
                        'class': row['class'],
                        'pos': pos,
                        'context': text[:30] + '...' if len(text) > 30 else text
                    })
            except (AttributeError, IndexError) as e:
                # ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
    
    return all_words, word_categories, word_contexts

def classify_vocabulary(all_words, word_contexts):
    """èªå½™ã‚’æ•™è‚²çš„ä¾¡å€¤ã§åˆ†é¡"""
    word_freq = Counter(all_words)
    
    # æ•™è‚²çš„èªå½™åˆ†é¡
    scientific_terms = []      # ç§‘å­¦ç”¨èª
    educational_terms = []     # æ•™è‚²é–¢é€£èª
    emotion_terms = []        # æ„Ÿæƒ…èª
    noise_terms = []          # ãƒã‚¤ã‚ºèª
    greeting_terms = []       # æŒ¨æ‹¶èª
    
    # ç§‘å­¦ç”¨èªãƒ‘ã‚¿ãƒ¼ãƒ³
    science_patterns = [
        r'ãƒŠãƒˆãƒªã‚¦ãƒ ', r'å¡©åŒ–ãƒŠãƒˆãƒªã‚¦ãƒ ', r'Na\+?', r'NaCl',
        r'å¡©', r'é£Ÿå¡©', r'å¡©åˆ†',
        r'ç‚è‰²åå¿œ', r'å†çµæ™¶', r'å®Ÿé¨“', r'çµæ™¶',
        r'ãƒãƒªã‚¦ãƒ ', r'ã‚«ãƒ«ã‚·ã‚¦ãƒ ', r'ã‚¹ãƒˆãƒ­ãƒ³ãƒã‚¦ãƒ ',
        r'æ°´ã‚ˆã†æ¶²', r'ã¨ã‘ã‚‹', r'æº¶ã‘ã‚‹', r'æˆåˆ†'
    ]
    
    # æ„Ÿæƒ…ãƒ»è©•ä¾¡èªãƒ‘ã‚¿ãƒ¼ãƒ³  
    emotion_patterns = [
        r'é¢ç™½ã„', r'ãŠã‚‚ã—ã‚ã„', r'æ¥½ã—ã„', r'ãŸã®ã—ã„',
        r'ã™ã”ã„', r'ãã‚Œã„', r'å°è±¡', r'ã³ã£ãã‚Š',
        r'å¥½ã', r'èˆˆå‘³', r'æ„Ÿå‹•'
    ]
    
    # ãƒã‚¤ã‚ºèªãƒ‘ã‚¿ãƒ¼ãƒ³
    noise_patterns = [
        r'ã‚ã‚ŠãŒã¨ã†', r'ã‚ˆã‚Š', r'ã¸', r'ã‹ã‚‰', r'ã§ã™', r'ã¾ã™',
        r'æ±äº¬é«˜å°‚', r'ã¿ãªã•ã‚“', r'ä»Šæ—¥', r'ä¸€ç•ª',
        r'ã“ã¨', r'ã‚‚ã®', r'æ™‚', r'æ‰€', r'å ´æ‰€'
    ]
    
    # æŒ¨æ‹¶èªãƒ‘ã‚¿ãƒ¼ãƒ³
    greeting_patterns = [
        r'ã‚ã‚ŠãŒã¨ã†', r'ãŠç–²ã‚Œ', r'ã‚ˆã‚ã—ã',
        r'ã“ã‚“ã«ã¡ã¯', r'ã•ã‚ˆã†ãªã‚‰'
    ]
    
    for word, freq in word_freq.most_common():
        classified = False
        
        # ç§‘å­¦ç”¨èªåˆ¤å®š
        for pattern in science_patterns:
            if re.search(pattern, word):
                scientific_terms.append((word, freq))
                classified = True
                break
        
        if not classified:
            # æ„Ÿæƒ…èªåˆ¤å®š
            for pattern in emotion_patterns:
                if re.search(pattern, word):
                    emotion_terms.append((word, freq))
                    classified = True
                    break
        
        if not classified:
            # ãƒã‚¤ã‚ºèªåˆ¤å®š
            for pattern in noise_patterns:
                if re.search(pattern, word):
                    noise_terms.append((word, freq))
                    classified = True
                    break
        
        if not classified:
            # æŒ¨æ‹¶èªåˆ¤å®š
            for pattern in greeting_patterns:
                if re.search(pattern, word):
                    greeting_terms.append((word, freq))
                    classified = True
                    break
        
        if not classified:
            # ãã®ä»–ã¯æ•™è‚²é–¢é€£ã¨ã—ã¦åˆ†é¡
            educational_terms.append((word, freq))
    
    return {
        'scientific': scientific_terms,
        'educational': educational_terms,
        'emotion': emotion_terms,
        'noise': noise_terms,
        'greeting': greeting_terms
    }

def find_normalization_candidates(all_words):
    """è¡¨è¨˜ã‚†ã‚Œå€™è£œã‚’æ¤œå‡º"""
    word_freq = Counter(all_words)
    
    # è¡¨è¨˜ã‚†ã‚Œå€™è£œã‚°ãƒ«ãƒ¼ãƒ—
    normalization_groups = {
        'ã¿ãç³»': ['ã¿ã', 'ã¿ãæ±', 'ã¿ãã—ã‚‹', 'å‘³å™Œ', 'å‘³å™Œæ±'],
        'é¢ç™½ã„ç³»': ['é¢ç™½ã„', 'ãŠã‚‚ã—ã‚ã„', 'ãŠã‚‚ã—ã‚ã'],
        'æ¥½ã—ã„ç³»': ['æ¥½ã—ã„', 'ãŸã®ã—ã„', 'ãŸã®ã—ã'],
        'å¡©ç³»': ['å¡©', 'é£Ÿå¡©', 'å¡©åˆ†', 'ãˆã‚“åˆ†'],
        'ç§‘å­¦ç³»': ['ãƒŠãƒˆãƒªã‚¦ãƒ ', 'å¡©åŒ–ãƒŠãƒˆãƒªã‚¦ãƒ ', 'Na', 'NaCl'],
        'å®Ÿé¨“ç³»': ['å®Ÿé¨“', 'ç‚è‰²åå¿œ', 'å†çµæ™¶', 'çµæ™¶'],
        'æº¶ã‘ã‚‹ç³»': ['ã¨ã‘ã‚‹', 'æº¶ã‘ã‚‹', 'ã¨ã‘ãŸ', 'æº¶ã‘ãŸ']
    }
    
    detected_groups = {}
    for group_name, candidates in normalization_groups.items():
        found_words = []
        for word in candidates:
            if word in word_freq:
                found_words.append((word, word_freq[word]))
        if found_words:
            detected_groups[group_name] = found_words
    
    return detected_groups

def analyze_educational_progression():
    """æˆæ¥­å‰å¾Œã®èªå½™å¤‰åŒ–åˆ†æï¼ˆæ•™è‚²åŠ¹æœæ¸¬å®šï¼‰"""
    df = pd.read_csv('data/processed/all_text_corpus.csv')
    
    before_texts = df[df['category'] == 'Q2ç†ç”±_æˆæ¥­å‰']['text'].tolist()
    after_texts = df[df['category'] == 'Q2ç†ç”±_æˆæ¥­å¾Œ']['text'].tolist()
    
    tokenizer = Tokenizer()
    
    def extract_key_terms(texts):
        terms = []
        for text in texts:
            tokens = tokenizer.tokenize(str(text))
            for token in tokens:
                word = token.surface
                if word in ['å¡©', 'ãƒŠãƒˆãƒªã‚¦ãƒ ', 'é£Ÿå¡©', 'å¡©åŒ–ãƒŠãƒˆãƒªã‚¦ãƒ ', 'Na']:
                    terms.append(word)
        return Counter(terms)
    
    before_terms = extract_key_terms(before_texts)
    after_terms = extract_key_terms(after_texts)
    
    print("\nğŸ¯ **æ•™è‚²åŠ¹æœåˆ†æï¼ˆæ ¸å¿ƒèªå½™ã®å¤‰åŒ–ï¼‰**")
    print("æˆæ¥­å‰:", dict(before_terms))
    print("æˆæ¥­å¾Œ:", dict(after_terms))
    
    # ãƒŠãƒˆãƒªã‚¦ãƒ ä½¿ç”¨ç‡
    before_total = sum(before_terms.values())
    after_total = sum(after_terms.values())
    
    if before_total > 0:
        before_na_rate = before_terms.get('ãƒŠãƒˆãƒªã‚¦ãƒ ', 0) / before_total * 100
    else:
        before_na_rate = 0
        
    if after_total > 0:
        after_na_rate = after_terms.get('ãƒŠãƒˆãƒªã‚¦ãƒ ', 0) / after_total * 100
    else:
        after_na_rate = 0
    
    print(f"ãƒŠãƒˆãƒªã‚¦ãƒ ä½¿ç”¨ç‡: {before_na_rate:.1f}% â†’ {after_na_rate:.1f}%")
    print(f"æ•™è‚²åŠ¹æœ: +{after_na_rate - before_na_rate:.1f}ãƒã‚¤ãƒ³ãƒˆ")

def main():
    """ãƒ¡ã‚¤ãƒ³åˆ†æå®Ÿè¡Œ"""
    print("ğŸ” **ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æ”¹å–„ã®ãŸã‚ã®èªå½™åˆ†æ**")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_and_analyze_data()
    
    # å½¢æ…‹ç´ è§£æ
    print("\nğŸ“ å½¢æ…‹ç´ è§£æå®Ÿè¡Œä¸­...")
    all_words, word_categories, word_contexts = tokenize_and_analyze(df)
    
    # èªå½™åˆ†é¡
    print("\nğŸ·ï¸ èªå½™åˆ†é¡å®Ÿè¡Œä¸­...")
    vocab_classification = classify_vocabulary(all_words, word_contexts)
    
    # çµæœè¡¨ç¤º
    print("\nğŸ“Š **èªå½™åˆ†é¡çµæœ**")
    for category, words in vocab_classification.items():
        print(f"\n{category.upper()}èªå½™ ({len(words)}èª):")
        for word, freq in sorted(words, key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {word}: {freq}å›")
    
    # è¡¨è¨˜ã‚†ã‚Œåˆ†æ
    print("\nğŸ”„ **è¡¨è¨˜ã‚†ã‚Œåˆ†æ**")
    normalization_groups = find_normalization_candidates(all_words)
    for group_name, words in normalization_groups.items():
        print(f"\n{group_name}:")
        for word, freq in words:
            print(f"  {word}: {freq}å›")
    
    # æ•™è‚²åŠ¹æœåˆ†æ
    analyze_educational_progression()
    
    # æ”¹å–„ææ¡ˆ
    print("\nğŸ’¡ **æ”¹å–„ææ¡ˆ**")
    print("1. ãƒã‚¤ã‚ºèªé™¤å¤–:", [w[0] for w in vocab_classification['noise'][:5]])
    print("2. æŒ¨æ‹¶èªé™¤å¤–:", [w[0] for w in vocab_classification['greeting'][:3]])
    print("3. ç§‘å­¦ç”¨èªå¼·èª¿:", [w[0] for w in vocab_classification['scientific'][:5]])
    print("4. è¡¨è¨˜çµ±ä¸€å¿…è¦:", list(normalization_groups.keys()))

if __name__ == "__main__":
    main()