#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿é©å¿œå‹ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
ã‚«ãƒ†ã‚´ãƒªã¨ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãå‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
"""

import pandas as pd
import re
from collections import Counter
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import font_manager

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAGothic', 'VL Gothic', 'Noto Sans CJK JP']

class AdaptiveWordCloudOptimizer:
    def __init__(self):
        self.tokenizer = Tokenizer()
        
        # å…±é€šãƒã‚¤ã‚ºèª
        self.noise_words = {
            'ã‹ã‚‰', 'ã§ã™', 'ã‚ã‚ŠãŒã¨ã†', 'æ±äº¬é«˜å°‚', 'ã¸', 'ã¿ãªã•ã‚“', 
            'ä»Šæ—¥', 'ã‚ˆã‚Š', 'ã“ã¨', 'ä¸€ç•ª', 'ã¾ã™', 'ã§ã—', 'ã¾ã—',
            'ãŒ', 'ã¦', 'ã„ã‚‹', 'ãŸ', 'ã®', 'ã«', 'ã¯', 'ã‚’', 'ã§',
            'å…¥ã£', 'ã—', 'ãªã£', 'ã£', 'ã„', 'ã‚“', 'ã‚‹', 'ã‚Œ'
        }
        
        # ç§‘å­¦ç”¨èªé‡ã¿ï¼ˆå…±é€šï¼‰
        self.science_terms = {
            'ãƒŠãƒˆãƒªã‚¦ãƒ ': 3.0, 'å¡©': 2.5, 'é£Ÿå¡©': 2.5, 'å¡©åˆ†': 2.5,
            'çµæ™¶': 2.8, 'å®Ÿé¨“': 2.8, 'ç‚è‰²åå¿œ': 3.0, 'å†çµæ™¶': 3.0,
            'æˆåˆ†': 2.0, 'å¡©åŒ–ãƒŠãƒˆãƒªã‚¦ãƒ ': 3.0, 'Na': 3.0, 'NaCl': 3.0
        }
        
        # æ„Ÿæƒ…èªé‡ã¿ï¼ˆå…±é€šï¼‰
        self.emotion_terms = {
            'å°è±¡': 2.0, 'ãã‚Œã„': 2.0, 'é¢ç™½ã„': 2.2, 'ãŠã‚‚ã—ã‚ã„': 2.2,
            'æ¥½ã—ã„': 2.2, 'å¥½ã': 2.0, 'èˆˆå‘³': 2.5, 'ã³ã£ãã‚Š': 2.0, 'ã™ã”ã„': 2.0
        }
        
        # è¡¨è¨˜ã‚†ã‚Œçµ±ä¸€ï¼ˆå…±é€šï¼‰
        self.normalization_rules = {
            'ã¿ãæ±': 'ã¿ã', 'ã¿ãã—ã‚‹': 'ã¿ã', 'å‘³å™Œ': 'ã¿ã', 'å‘³å™Œæ±': 'ã¿ã',
            'ãŠã‚‚ã—ã‚ã„': 'é¢ç™½ã„', 'ãŠã‚‚ã—ã‚ã': 'é¢ç™½ã„', 'ãŠã‚‚ã—ã‚ã‹ã£': 'é¢ç™½ã„',
            'ãŸã®ã—ã„': 'æ¥½ã—ã„', 'ãŸã®ã—ã': 'æ¥½ã—ã„', 'ãŸã®ã—ã‹ã£': 'æ¥½ã—ã„',
            'ãˆã‚“åˆ†': 'å¡©åˆ†', 'ã‚¨ãƒ³åˆ†': 'å¡©åˆ†',
            'ã¨ã‘ã‚‹': 'æº¶ã‘ã‚‹', 'ã¨ã‘': 'æº¶ã‘ã‚‹', 'ã¨ã‹ã—': 'æº¶ã‘ã‚‹', 'æº¶ã‹ã—': 'æº¶ã‘ã‚‹'
        }
    
    def detect_dataset_type(self, df):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã®è‡ªå‹•æ¤œå‡º"""
        category_counts = df['category'].value_counts()
        
        if 'æ„Ÿæƒ³æ–‡' in category_counts and category_counts['æ„Ÿæƒ³æ–‡'] > 0:
            if len(category_counts) == 1:
                return 'comments_only'
            else:
                return 'mixed'
        elif 'Q2ç†ç”±_æˆæ¥­å‰' in category_counts and 'Q2ç†ç”±_æˆæ¥­å¾Œ' in category_counts:
            return 'reasoning_only'
        elif 'Q2ç†ç”±_æˆæ¥­å‰' in category_counts:
            return 'before_only'
        elif 'Q2ç†ç”±_æˆæ¥­å¾Œ' in category_counts:
            return 'after_only'
        else:
            return 'unknown'
    
    def get_adaptive_parameters(self, df, dataset_type):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã«åŸºã¥ãé©å¿œå‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
        
        # èªå½™æ•°åˆ†æ
        words = self.extract_and_filter_words(df)
        word_freq = Counter([word for word, category in words])
        unique_words = len(word_freq)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾©
        if dataset_type == 'comments_only':
            # æ„Ÿæƒ³æ–‡å°‚ç”¨ï¼šå¤šèªå½™ç¶²ç¾…å‹
            params = {
                'max_words': min(unique_words, 140),
                'min_font_size': 20,
                'max_font_size': 160,
                'prefer_horizontal': 0.9,
                'relative_scaling': 0.3,
                'width': 1400,
                'height': 800,
                'colormap': 'viridis',
                'background_color': '#f8f8f8'
            }
        elif dataset_type in ['reasoning_only', 'before_only', 'after_only']:
            # ç†ç”±èª¬æ˜å°‚ç”¨ï¼šå°‘èªå½™é›†ä¸­å‹
            params = {
                'max_words': min(unique_words, 50),  # å¤§å¹…å‰Šæ¸›
                'min_font_size': 32,  # å¤§å‹ãƒ•ã‚©ãƒ³ãƒˆ
                'max_font_size': 220,  # è¶…å¤§å‹ãƒ•ã‚©ãƒ³ãƒˆ
                'prefer_horizontal': 0.75,  # ç¸¦æ›¸ãè¨±å®¹
                'relative_scaling': 0.6,  # å¼·ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                'width': 1000,  # ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆå¹…
                'height': 600,  # ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆé«˜ã•
                'colormap': 'plasma',
                'background_color': '#f5f5f5'
            }
        else:  # mixed
            # çµ±åˆå‹ï¼šãƒãƒ©ãƒ³ã‚¹èª¿æ•´
            params = {
                'max_words': min(unique_words, 100),
                'min_font_size': 24,
                'max_font_size': 180,
                'prefer_horizontal': 0.85,
                'relative_scaling': 0.4,
                'width': 1200,
                'height': 700,
                'colormap': 'coolwarm',
                'background_color': '#f8f8f8'
            }
        
        return params, unique_words
    
    def extract_and_filter_words(self, df):
        """èªå½™æŠ½å‡ºãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå…±é€šå‡¦ç†ï¼‰"""
        filtered_words = []
        
        for idx, row in df.iterrows():
            text = str(row['text'])
            category = row['category']
            
            # å‰å‡¦ç†
            text = re.sub(r'[0-9ï¼-ï¼™]', '', text)
            text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿãƒ»ã€Œã€ã€ã€ï¼ˆï¼‰()]', '', text)
            
            # å½¢æ…‹ç´ è§£æ
            tokens = self.tokenizer.tokenize(text)
            for token in tokens:
                try:
                    word = token.surface
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if (len(word) >= 2 and 
                        word not in self.noise_words and
                        not re.match(r'^[ã-ã‚“]{1}$', word)):
                        
                        # è¡¨è¨˜ã‚†ã‚Œçµ±ä¸€
                        if word in self.normalization_rules:
                            word = self.normalization_rules[word]
                        
                        filtered_words.append((word, category))
                        
                except (AttributeError, IndexError):
                    continue
        
        return filtered_words
    
    def calculate_weighted_frequencies(self, words):
        """é‡ã¿ä»˜ãé »åº¦è¨ˆç®—ï¼ˆå…±é€šå‡¦ç†ï¼‰"""
        word_freq = Counter([word for word, category in words])
        weighted_freq = {}
        
        for word, freq in word_freq.items():
            base_weight = freq
            
            # ç§‘å­¦ç”¨èªé‡ã¿å¢—åŠ 
            if word in self.science_terms:
                weight = base_weight * self.science_terms[word]
            # æ„Ÿæƒ…èªé‡ã¿å¢—åŠ 
            elif word in self.emotion_terms:
                weight = base_weight * self.emotion_terms[word]
            else:
                weight = base_weight
            
            weighted_freq[word] = weight
        
        return weighted_freq
    
    def generate_adaptive_wordcloud(self, df, title_suffix=""):
        """é©å¿œå‹ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ"""
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—æ¤œå‡º
        dataset_type = self.detect_dataset_type(df)
        print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—: {dataset_type}")
        
        # é©å¿œå‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—
        params, unique_words = self.get_adaptive_parameters(df, dataset_type)
        print(f"ğŸ“Š èªå½™æ•°: {unique_words}èª")
        print(f"âš™ï¸ é©ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: max_words={params['max_words']}, font_size={params['min_font_size']}-{params['max_font_size']}")
        
        # èªå½™å‡¦ç†
        words = self.extract_and_filter_words(df)
        weighted_freq = self.calculate_weighted_frequencies(words)
        
        # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        font_path = 'fonts/ipaexg.ttf'
        
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ
        wordcloud = WordCloud(
            font_path=font_path,
            width=params['width'],
            height=params['height'],
            background_color=params['background_color'],
            max_words=params['max_words'],
            min_font_size=params['min_font_size'],
            max_font_size=params['max_font_size'],
            prefer_horizontal=params['prefer_horizontal'],
            relative_scaling=params['relative_scaling'],
            colormap=params['colormap']
        )
        
        if weighted_freq:
            wordcloud.generate_from_frequencies(weighted_freq)
            
            # ã‚¿ã‚¤ãƒˆãƒ«è¨­å®š
            title = f"é©å¿œå‹ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ ({dataset_type}){title_suffix}"
            
            # å¯è¦–åŒ–
            plt.figure(figsize=(params['width']/100, params['height']/100))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title(title, fontsize=16, fontweight='bold', pad=20)
            plt.tight_layout()
            
            return wordcloud, weighted_freq, params, dataset_type
        else:
            print("âš ï¸ æœ‰åŠ¹ãªèªå½™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None, {}, {}, dataset_type

def generate_comparison_wordclouds():
    """æ¯”è¼ƒç”¨ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ"""
    print("ğŸ¨ **é©å¿œå‹ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æ¯”è¼ƒç”Ÿæˆ**")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_csv('data/processed/all_text_corpus.csv')
    optimizer = AdaptiveWordCloudOptimizer()
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ç”Ÿæˆ
    categories = {
        'æ„Ÿæƒ³æ–‡': df[df['category'] == 'æ„Ÿæƒ³æ–‡'],
        'æˆæ¥­å‰ç†ç”±': df[df['category'] == 'Q2ç†ç”±_æˆæ¥­å‰'],
        'æˆæ¥­å¾Œç†ç”±': df[df['category'] == 'Q2ç†ç”±_æˆæ¥­å¾Œ'],
        'å…¨ãƒ‡ãƒ¼ã‚¿': df
    }
    
    results = {}
    
    for name, data in categories.items():
        print(f"\n{'='*20} {name} ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ {'='*20}")
        
        wordcloud, frequencies, params, dataset_type = optimizer.generate_adaptive_wordcloud(
            data, title_suffix=f" - {name}"
        )
        
        if wordcloud:
            # ä¿å­˜
            output_path = f'outputs/wordclouds/adaptive_{name.replace("ç†ç”±", "reason")}.png'
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_path}")
            
            # ãƒˆãƒƒãƒ—èªå½™è¡¨ç¤º
            print(f"ğŸ† ãƒˆãƒƒãƒ—èªå½™ (ä¸Šä½10èª):")
            for word, weight in sorted(frequencies.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {word}: {weight:.1f}")
            
            results[name] = {
                'wordcloud': wordcloud,
                'frequencies': frequencies,
                'parameters': params,
                'dataset_type': dataset_type
            }
            
            plt.close()  # ãƒ¡ãƒ¢ãƒªç¯€ç´„
    
    return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    results = generate_comparison_wordclouds()
    
    print(f"\nâœ… **é©å¿œå‹æœ€é©åŒ–å®Œäº†**")
    print("ğŸ¯ **åŠ¹æœçš„ãªå¯¾ç­–å®Ÿè£…**:")
    print("  1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—è‡ªå‹•æ¤œå‡º")
    print("  2. èªå½™æ•°ã«å¿œã˜ãŸå‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´")
    print("  3. ã‚«ãƒ†ã‚´ãƒªæœ€é©åŒ–æ¸ˆã¿ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ")
    print("  4. ç†ç”±èª¬æ˜ç”¨ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè¨­è¨ˆ")

if __name__ == "__main__":
    main()