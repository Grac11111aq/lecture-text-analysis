# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰
## æ±äº¬é«˜å°‚å‡ºå‰æˆæ¥­ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°åˆ†æç’°å¢ƒ

**æœ€çµ‚æ›´æ–°**: 2025-05-31  
**å¯¾è±¡OS**: Linux (WSL2), macOS, Windows  
**Pythonè¦ä»¶**: 3.8+  

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. åŸºæœ¬ç’°å¢ƒç¢ºèª
```bash
# Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
python --version  # 3.8+ å¿…é ˆ

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd /home/grace/projects/social-implement/lecture-survey-analysis/lecture-text-analysis

# ä»®æƒ³ç’°å¢ƒä½œæˆï¼ˆæ¨å¥¨ï¼‰
python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate  # Windows
```

### 2. ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install --upgrade pip
pip install -r requirements.txt

# è¿½åŠ åˆ†æãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install fugashi unidic-lite plotly kaleido pingouin statsmodels
```

### 3. æ—¥æœ¬èªNLPç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### Option A: MeCabï¼ˆé«˜ç²¾åº¦ã€æ¨©é™å•é¡Œã®å¯èƒ½æ€§ï¼‰
```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install mecab mecab-ipadic-utf8 libmecab-dev

# macOS
brew install mecab mecab-ipadic

# Python ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
pip install mecab-python3
```

**æ¨©é™å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆ**: Option Bï¼ˆjanomeï¼‰ã‚’ä½¿ç”¨

#### Option B: janomeï¼ˆç´”Pythonã€æ¨©é™å•é¡Œãªã—ï¼‰
```bash
# janomeã¯ requirements.txt ã«å«æœ‰æ¸ˆã¿
# è¨­å®šä¸è¦ã€å³åˆ©ç”¨å¯èƒ½
```

### 4. ç’°å¢ƒæ¤œè¨¼
```bash
# æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
python scripts/setup/validate_environment.py
```

## ğŸ“¦ è©³ç´°ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †

### ä¾å­˜é–¢ä¿‚ã®è©³ç´°èª¬æ˜

#### å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
```txt
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»åŸºæœ¬åˆ†æ
pandas>=2.0.0           # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ“ä½œ
numpy>=1.24.0            # æ•°å€¤è¨ˆç®—
scipy>=1.11.0            # çµ±è¨ˆè¨ˆç®—

# è‡ªç„¶è¨€èªå‡¦ç†
janome>=0.5.0            # æ—¥æœ¬èªå½¢æ…‹ç´ è§£æï¼ˆç´”Pythonï¼‰
spacy>=3.5.0             # NLP pipeline
ginza>=5.1.0             # æ—¥æœ¬èªspaCyãƒ¢ãƒ‡ãƒ«

# å¯è¦–åŒ–
matplotlib>=3.7.0        # åŸºæœ¬å›³è¡¨
seaborn>=0.12.0          # çµ±è¨ˆçš„å¯è¦–åŒ–
wordcloud>=1.9.0         # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰

# æ©Ÿæ¢°å­¦ç¿’
scikit-learn>=1.3.0      # æ©Ÿæ¢°å­¦ç¿’ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
gensim>=4.3.0            # ãƒˆãƒ”ãƒƒã‚¯ãƒ¢ãƒ‡ãƒªãƒ³ã‚°

# çµ±è¨ˆåˆ†æ
textblob>=0.17.0         # æ„Ÿæƒ…åˆ†æ
```

#### æ‹¡å¼µãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
```txt
# é«˜åº¦çµ±è¨ˆåˆ†æ
pingouin>=0.5.3          # çµ±è¨ˆæ¤œå®šãƒ»åŠ¹æœé‡
statsmodels>=0.14.0      # å›å¸°åˆ†æãƒ»æ™‚ç³»åˆ—

# é«˜åº¦å¯è¦–åŒ–
plotly>=5.15.0           # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å›³è¡¨
kaleido>=0.2.1           # é™çš„ç”»åƒå‡ºåŠ›

# æ—¥æœ¬èªå‡¦ç†å¼·åŒ–
fugashi>=1.3.0           # MeCabãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆé«˜é€Ÿï¼‰
unidic-lite>=1.0.8       # UniDicè¾æ›¸ï¼ˆè»½é‡ç‰ˆï¼‰
```

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

#### 1. MeCab ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—
**ç—‡çŠ¶**: `sudo: command not found` ã¾ãŸã¯æ¨©é™ã‚¨ãƒ©ãƒ¼
```bash
# è§£æ±ºæ³•1: janomeä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
# requirements.txt ã®janomeã‚’ä½¿ç”¨ã€è¨­å®šå¤‰æ›´ä¸è¦

# è§£æ±ºæ³•2: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ãƒ™ãƒ«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ¨©é™å•é¡Œå›é¿ï¼‰
# conda/minicondaä½¿ç”¨
conda install -c conda-forge mecab mecab-ipadic

# è§£æ±ºæ³•3: æ‰‹å‹•æ¨©é™è¦è«‹
echo "MeCab installation requires system privileges. Please request manual installation."
```

#### 2. spaCy/ginza ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—
```bash
# æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
python -m spacy download ja_ginza
python -m spacy download ja_ginza_electra

# ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒå¯¾å¿œ
pip install --proxy http://proxy.server:port ja-ginza
```

#### 3. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼
```bash
# å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ™‚ã®å¯¾ç­–
export PYTHONHASHSEED=0
ulimit -v 4194304  # ãƒ¡ãƒ¢ãƒªåˆ¶é™è¨­å®šï¼ˆ4GBï¼‰

# ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
# config/analysis_config.yaml å†…ã§ chunk_size ã‚’å‰Šæ¸›
```

#### 4. æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œ
```bash
# ç’°å¢ƒå¤‰æ•°è¨­å®š
export LANG=ja_JP.UTF-8
export LC_ALL=ja_JP.UTF-8

# Pythonå†…ã§ã®è¨­å®šç¢ºèª
python -c "import locale; print(locale.getpreferredencoding())"
```

## ğŸ§ª ç’°å¢ƒæ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

### è‡ªå‹•æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
**scripts/setup/validate_environment.py** ã®ä½œæˆ:
```python
#!/usr/bin/env python3
"""
ç’°å¢ƒæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿè¡Œ: python scripts/setup/validate_environment.py
"""

import sys
import importlib
import subprocess
from pathlib import Path

def check_python_version():
    """Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª"""
    version = sys.version_info
    if version.major == 3 and version.minor >= 8:
        print(f"âœ“ Python {version.major}.{version.minor}.{version.micro} - OK")
        return True
    else:
        print(f"âœ— Python {version.major}.{version.minor}.{version.micro} - 3.8+ required")
        return False

def check_package(package_name, import_name=None):
    """ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å­˜åœ¨ç¢ºèª"""
    if import_name is None:
        import_name = package_name
    
    try:
        module = importlib.import_module(import_name)
        version = getattr(module, '__version__', 'unknown')
        print(f"âœ“ {package_name} {version} - OK")
        return True
    except ImportError:
        print(f"âœ— {package_name} - NOT FOUND")
        return False

def check_japanese_nlp():
    """æ—¥æœ¬èªNLPç’°å¢ƒç¢ºèª"""
    # janome ãƒ†ã‚¹ãƒˆ
    try:
        from janome.tokenizer import Tokenizer
        tokenizer = Tokenizer()
        tokens = list(tokenizer.tokenize("ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™"))
        print("âœ“ janome - Japanese tokenization OK")
        return True
    except:
        print("âœ— janome - Japanese tokenization FAILED")
        return False

def check_data_files():
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª"""
    required_files = [
        "data/raw/comments.csv",
        "data/raw/q2_reasons_before.csv", 
        "data/raw/q2_reasons_after.csv"
    ]
    
    all_exist = True
    for file_path in required_files:
        if Path(file_path).exists():
            print(f"âœ“ {file_path} - EXISTS")
        else:
            print(f"âœ— {file_path} - NOT FOUND")
            all_exist = False
    
    return all_exist

def check_output_directories():
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª"""
    required_dirs = [
        "outputs/wordclouds",
        "outputs/sentiment_results",
        "outputs/topic_models"
    ]
    
    for dir_path in required_dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
        print(f"âœ“ {dir_path} - READY")

def main():
    """ãƒ¡ã‚¤ãƒ³æ¤œè¨¼å®Ÿè¡Œ"""
    print("=== ç’°å¢ƒæ¤œè¨¼é–‹å§‹ ===\n")
    
    # Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    python_ok = check_python_version()
    
    # å¿…é ˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
    packages = [
        "pandas", "numpy", "scipy", "matplotlib", "seaborn",
        "sklearn", "gensim", "janome", "textblob", "wordcloud"
    ]
    
    packages_ok = all(check_package(pkg) for pkg in packages)
    
    # æ—¥æœ¬èªNLP
    nlp_ok = check_japanese_nlp()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
    data_ok = check_data_files()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    check_output_directories()
    
    # ç·åˆåˆ¤å®š
    if all([python_ok, packages_ok, nlp_ok, data_ok]):
        print("\nâœ“ å…¨ç’°å¢ƒç¢ºèªå®Œäº† - åˆ†æå®Ÿè¡Œå¯èƒ½")
        return True
    else:
        print("\nâœ— ç’°å¢ƒä¸å‚™ã‚ã‚Š - ä¸Šè¨˜ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ãã ã•ã„")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
```

### æ®µéšçš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# Phase 1: åŸºæœ¬ç’°å¢ƒ
python scripts/setup/validate_environment.py

# Phase 2: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
python -c "
import pandas as pd
df = pd.read_csv('data/raw/comments.csv')
print(f'Comments loaded: {len(df)} records')
"

# Phase 3: æ—¥æœ¬èªå‡¦ç†ãƒ†ã‚¹ãƒˆ
python -c "
from janome.tokenizer import Tokenizer
t = Tokenizer()
result = list(t.tokenize('ã¿ãæ±ã«ãƒŠãƒˆãƒªã‚¦ãƒ ãŒå…¥ã£ã¦ã„ã‚‹ã‹ã‚‰'))
print('Japanese tokenization successful')
"

# Phase 4: å¯è¦–åŒ–ãƒ†ã‚¹ãƒˆ
python -c "
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(6,4))
sns.barplot(x=[1,2,3], y=[1,2,3])
plt.savefig('outputs/test_plot.png')
print('Visualization test successful')
"
```

## ğŸ”§ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ

### åˆ†æè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
**config/analysis_config.yaml**:
```yaml
# åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
data:
  encoding: "utf-8"
  chunk_size: 1000
  
vocabulary_analysis:
  target_terms:
    - "ã¿ã"
    - "å¡©" 
    - "é£Ÿå¡©"
    - "ãƒŠãƒˆãƒªã‚¦ãƒ "
    - "å¡©åŒ–ãƒŠãƒˆãƒªã‚¦ãƒ "
  
  scientific_terms:
    basic: ["å¡©", "é£Ÿå¡©"]
    advanced: ["ãƒŠãƒˆãƒªã‚¦ãƒ ", "å¡©åŒ–ãƒŠãƒˆãƒªã‚¦ãƒ ", "Na"]
  
  analysis_params:
    significance_level: 0.05
    effect_size_threshold: 0.2
    confidence_interval: 0.95

class_analysis:
  classes: [1.0, 2.0, 3.0, 4.0]
  multiple_comparison: "bonferroni"
  min_sample_size: 5

sentiment_analysis:
  language: "japanese"
  polarity_threshold: 0.1
  interest_keywords:
    - "ãŠã‚‚ã—ã‚ã„"
    - "ã™ã”ã„" 
    - "ãã‚Œã„"
    - "æ¥½ã—ã„"

topic_modeling:
  algorithm: "lda"
  n_topics: 5
  alpha: "auto"
  random_state: 42
  max_iterations: 1000

visualization:
  style: "seaborn-v0_8"
  figure_size: [12, 8]
  font_size: 12
  color_palette: "viridis"
  save_format: "png"
  dpi: 300
```

### ãƒ‘ã‚¹ç®¡ç†è¨­å®š
**config/paths.yaml**:
```yaml
# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
data:
  raw_dir: "data/raw"
  processed_dir: "data/processed" 
  comments: "data/raw/comments.csv"
  q2_before: "data/raw/q2_reasons_before.csv"
  q2_after: "data/raw/q2_reasons_after.csv"

outputs:
  base_dir: "outputs"
  wordclouds: "outputs/wordclouds"
  sentiment: "outputs/sentiment_results"
  topics: "outputs/topic_models"
  statistics: "outputs/statistics"
  visualizations: "outputs/visualizations"

scripts:
  analysis_dir: "scripts/analysis"
  utils_dir: "scripts/utils"
  
logs:
  main_log: "logs/analysis.log"
  error_log: "logs/errors.log"
```

## ğŸ“‹ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å¿…é ˆé …ç›®
- [ ] Python 3.8+ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
- [ ] requirements.txt ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å°å…¥å®Œäº†
- [ ] æ—¥æœ¬èªNLPç’°å¢ƒï¼ˆjanome ã¾ãŸã¯ MeCabï¼‰å‹•ä½œç¢ºèª
- [ ] ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«3ã¤ã®å­˜åœ¨ç¢ºèª
- [ ] å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆç¢ºèª

### æ¨å¥¨é …ç›®  
- [ ] ä»®æƒ³ç’°å¢ƒã®ä½œæˆãƒ»ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
- [ ] æ‹¡å¼µãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆpingouin, plotlyç­‰ï¼‰å°å…¥
- [ ] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆYAMLï¼‰ä½œæˆ
- [ ] ç’°å¢ƒæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
- [ ] ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã«ã‚ˆã‚‹å‹•ä½œç¢ºèª

### æ¨©é™å•é¡Œå¯¾å¿œ
- [ ] MeCab ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¯èƒ½æ€§ç¢ºèª
- [ ] ä»£æ›¿æ‰‹æ³•ï¼ˆjanomeï¼‰æº–å‚™ç¢ºèª
- [ ] ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã¸ã®ç›¸è«‡æº–å‚™ï¼ˆå¿…è¦æ™‚ï¼‰

## ğŸ†˜ ã‚µãƒãƒ¼ãƒˆãƒ»ãƒˆãƒ©ãƒ–ãƒ«å¯¾å¿œ

### æ¨©é™å•é¡Œç™ºç”Ÿæ™‚ã®é€£çµ¡äº‹é …
```
ä»¶å: ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— - æ¨©é™æ”¯æ´è¦è«‹

å¿…è¦ãªä½œæ¥­:
1. MeCabè¾æ›¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: sudo apt-get install mecab mecab-ipadic-utf8
2. ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ›´æ–°: sudo apt-get update
3. ç’°å¢ƒå¤‰æ•°è¨­å®šã®æ”¯æ´

ä»£æ›¿æ‰‹æ³•: 
- janomeä½¿ç”¨ã«ã‚ˆã‚‹å›é¿ç­–æº–å‚™æ¸ˆã¿
- åˆ†æç²¾åº¦ã¸ã®å½±éŸ¿ã¯é™å®šçš„

ç·Šæ€¥åº¦: ä¸­ï¼ˆä»£æ›¿æ‰‹æ³•ã«ã‚ˆã‚Šä½œæ¥­ç¶™ç¶šå¯èƒ½ï¼‰
```

### ã‚¨ãƒ©ãƒ¼å ±å‘Šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
```markdown
## ã‚¨ãƒ©ãƒ¼å ±å‘Š
**ç™ºç”Ÿæ—¥æ™‚**: 
**ä½œæ¥­ãƒ•ã‚§ãƒ¼ã‚º**: 
**ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸**: 
```ã‚³ãƒãƒ³ãƒ‰/ã‚¨ãƒ©ãƒ¼å†…å®¹```
**è©¦è¡Œã—ãŸè§£æ±ºç­–**: 
**ç’°å¢ƒæƒ…å ±**: OSã€Python versionã€etc.
```

---

**ä½œæˆè€…**: Claude Code Analysis  
**æ‰¿èª**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼  
**æ¬¡å›æ›´æ–°**: ç’°å¢ƒå•é¡Œç™ºç”Ÿæ™‚ã¾ãŸã¯æ–°è¦è¦ä»¶è¿½åŠ æ™‚