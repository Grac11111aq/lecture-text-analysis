#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ‡ãƒ¼ã‚¿é©å¿œå‹ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’åˆ†æã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æˆ¦ç•¥ã‚’ç­–å®š
"""

import pandas as pd
import re
from collections import Counter
from janome.tokenizer import Tokenizer
import numpy as np
import json

class DatasetAnalyzer:
    def __init__(self):
        self.tokenizer = Tokenizer()
        self.noise_words = {
            'ã‹ã‚‰', 'ã§ã™', 'ã‚ã‚ŠãŒã¨ã†', 'æ±äº¬é«˜å°‚', 'ã¸', 'ã¿ãªã•ã‚“', 
            'ä»Šæ—¥', 'ã‚ˆã‚Š', 'ã“ã¨', 'ä¸€ç•ª', 'ã¾ã™', 'ã§ã—', 'ã¾ã—',
            'ãŒ', 'ã¦', 'ã„ã‚‹', 'ãŸ', 'ã®', 'ã«', 'ã¯', 'ã‚’', 'ã§',
            'å…¥ã£', 'ã—', 'ãªã£', 'ã£', 'ã„', 'ã‚“', 'ã‚‹', 'ã‚Œ'
        }
    
    def analyze_by_category(self, df):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ"""
        results = {}
        
        for category in df['category'].unique():
            category_df = df[df['category'] == category]
            analysis = self.analyze_category_characteristics(category_df, category)
            results[category] = analysis
        
        return results
    
    def analyze_category_characteristics(self, df, category_name):
        """å€‹åˆ¥ã‚«ãƒ†ã‚´ãƒªã®ç‰¹æ€§åˆ†æ"""
        texts = df['text'].tolist()
        
        # åŸºæœ¬çµ±è¨ˆ
        total_records = len(texts)
        avg_text_length = np.mean([len(str(text)) for text in texts])
        
        # èªå½™åˆ†æ
        all_words = []
        filtered_words = []
        
        for text in texts:
            text = str(text)
            tokens = self.tokenizer.tokenize(text)
            
            for token in tokens:
                try:
                    word = token.surface
                    if len(word) >= 1:
                        all_words.append(word)
                        
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿èªå½™
                    if (len(word) >= 2 and 
                        word not in self.noise_words and
                        not re.match(r'^[ã-ã‚“]{1}$', word)):
                        filtered_words.append(word)
                        
                except (AttributeError, IndexError):
                    continue
        
        # èªå½™çµ±è¨ˆ
        word_freq = Counter(filtered_words)
        unique_words = len(word_freq)
        total_words = len(filtered_words)
        
        # èªå½™åˆ†å¸ƒåˆ†æ
        freq_values = list(word_freq.values())
        vocabulary_density = unique_words / max(total_words, 1)
        
        # é »åº¦åˆ†å¸ƒ
        freq_distribution = {
            'high_freq': len([f for f in freq_values if f >= 10]),  # 10å›ä»¥ä¸Š
            'medium_freq': len([f for f in freq_values if 3 <= f < 10]),  # 3-9å›
            'low_freq': len([f for f in freq_values if f < 3])  # 1-2å›
        }
        
        # ãƒˆãƒƒãƒ—èªå½™
        top_words = dict(word_freq.most_common(20))
        
        return {
            'basic_stats': {
                'records': total_records,
                'avg_text_length': round(avg_text_length, 1),
                'total_words': total_words,
                'unique_words': unique_words,
                'vocabulary_density': round(vocabulary_density, 3)
            },
            'frequency_distribution': freq_distribution,
            'top_words': top_words,
            'recommended_params': self.calculate_optimal_parameters(
                unique_words, freq_distribution, vocabulary_density
            )
        }
    
    def calculate_optimal_parameters(self, unique_words, freq_dist, density):
        """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—"""
        
        # åŸºæœ¬æ–¹é‡
        if unique_words < 30:  # å°‘èªå½™ï¼ˆç†ç”±èª¬æ˜ã‚¿ã‚¤ãƒ—ï¼‰
            params = {
                'max_words': min(unique_words * 2, 50),
                'min_font_size': 28,
                'max_font_size': 200,
                'prefer_horizontal': 0.8,
                'relative_scaling': 0.5,
                'width': 1000,
                'height': 600,
                'strategy': 'compact_focus'
            }
        elif unique_words < 80:  # ä¸­èªå½™
            params = {
                'max_words': min(unique_words * 1.5, 100),
                'min_font_size': 24,
                'max_font_size': 180,
                'prefer_horizontal': 0.85,
                'relative_scaling': 0.4,
                'width': 1200,
                'height': 700,
                'strategy': 'balanced'
            }
        else:  # å¤šèªå½™ï¼ˆæ„Ÿæƒ³æ–‡ã‚¿ã‚¤ãƒ—ï¼‰
            params = {
                'max_words': min(unique_words, 140),
                'min_font_size': 20,
                'max_font_size': 160,
                'prefer_horizontal': 0.9,
                'relative_scaling': 0.3,
                'width': 1400,
                'height': 800,
                'strategy': 'comprehensive'
            }
        
        # èªå½™å¯†åº¦ã«ã‚ˆã‚‹èª¿æ•´
        if density < 0.3:  # ä½å¯†åº¦ï¼ˆé‡è¤‡å¤šã„ï¼‰
            params['relative_scaling'] *= 1.2
            params['max_font_size'] = min(params['max_font_size'] * 1.1, 220)
        elif density > 0.7:  # é«˜å¯†åº¦ï¼ˆå¤šæ§˜æ€§é«˜ã„ï¼‰
            params['max_words'] = min(params['max_words'] * 1.2, 160)
        
        return params
    
    def generate_optimization_strategy(self, analysis_results):
        """æœ€é©åŒ–æˆ¦ç•¥ç”Ÿæˆ"""
        strategies = {}
        
        for category, data in analysis_results.items():
            basic_stats = data['basic_stats']
            params = data['recommended_params']
            
            if basic_stats['unique_words'] < 30:
                strategy_type = "å°‘èªå½™é›†ä¸­å‹"
                focus = "æ ¸å¿ƒèªå½™ã®å¼·èª¿ã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆé…ç½®"
                warnings = ["èªå½™æ•°ä¸è¶³", "å˜èª¿ãªè¡¨ç¤ºãƒªã‚¹ã‚¯"]
            elif basic_stats['unique_words'] < 80:
                strategy_type = "ä¸­èªå½™ãƒãƒ©ãƒ³ã‚¹å‹"
                focus = "é‡è¦èªå½™ã¨ã‚µãƒãƒ¼ãƒˆèªå½™ã®ãƒãƒ©ãƒ³ã‚¹"
                warnings = ["ä¸­ç¨‹åº¦ã®æƒ…å ±å¯†åº¦"]
            else:
                strategy_type = "å¤šèªå½™ç¶²ç¾…å‹"
                focus = "å¤šæ§˜ãªèªå½™ã®éšå±¤çš„è¡¨ç¤º"
                warnings = ["æƒ…å ±éå¤šãƒªã‚¹ã‚¯", "å°æ–‡å­—ã®å¯èª­æ€§"]
            
            strategies[category] = {
                'type': strategy_type,
                'focus': focus,
                'warnings': warnings,
                'params': params
            }
        
        return strategies

def main():
    """ãƒ¡ã‚¤ãƒ³åˆ†æå®Ÿè¡Œ"""
    print("ğŸ” **ãƒ‡ãƒ¼ã‚¿é©å¿œå‹ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰åˆ†æ**")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_csv('data/processed/all_text_corpus.csv')
    analyzer = DatasetAnalyzer()
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    print("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æä¸­...")
    analysis_results = analyzer.analyze_by_category(df)
    
    # çµæœè¡¨ç¤º
    for category, data in analysis_results.items():
        print(f"\n{'='*20} {category} {'='*20}")
        stats = data['basic_stats']
        print(f"ğŸ“ åŸºæœ¬çµ±è¨ˆ:")
        print(f"  ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {stats['records']}ä»¶")
        print(f"  å¹³å‡æ–‡å­—æ•°: {stats['avg_text_length']}æ–‡å­—")
        print(f"  ç·èªæ•°: {stats['total_words']}èª")
        print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯èªæ•°: {stats['unique_words']}èª")
        print(f"  èªå½™å¯†åº¦: {stats['vocabulary_density']}")
        
        freq_dist = data['frequency_distribution']
        print(f"\nğŸ“ˆ é »åº¦åˆ†å¸ƒ:")
        print(f"  é«˜é »åº¦èª(10å›+): {freq_dist['high_freq']}èª")
        print(f"  ä¸­é »åº¦èª(3-9å›): {freq_dist['medium_freq']}èª")
        print(f"  ä½é »åº¦èª(1-2å›): {freq_dist['low_freq']}èª")
        
        params = data['recommended_params']
        print(f"\nâš™ï¸ æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  æœ€å¤§èªæ•°: {params['max_words']}")
        print(f"  ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚º: {params['min_font_size']}-{params['max_font_size']}")
        print(f"  æ°´å¹³å„ªå…ˆåº¦: {params['prefer_horizontal']}")
        print(f"  ç›¸å¯¾ã‚¹ã‚±ãƒ¼ãƒ«: {params['relative_scaling']}")
        print(f"  ç”»åƒã‚µã‚¤ã‚º: {params['width']}x{params['height']}")
        print(f"  æˆ¦ç•¥: {params['strategy']}")
        
        print(f"\nğŸ† ãƒˆãƒƒãƒ—èªå½™ (ä¸Šä½10èª):")
        for word, freq in list(data['top_words'].items())[:10]:
            print(f"  {word}: {freq}å›")
    
    # æœ€é©åŒ–æˆ¦ç•¥
    print(f"\n{'='*20} æœ€é©åŒ–æˆ¦ç•¥ {'='*20}")
    strategies = analyzer.generate_optimization_strategy(analysis_results)
    
    for category, strategy in strategies.items():
        print(f"\nğŸ¯ {category}:")
        print(f"  æˆ¦ç•¥ã‚¿ã‚¤ãƒ—: {strategy['type']}")
        print(f"  é‡ç‚¹: {strategy['focus']}")
        print(f"  æ³¨æ„ç‚¹: {', '.join(strategy['warnings'])}")
    
    # å¯¾ç­–ææ¡ˆ
    print(f"\n{'='*20} å¯¾ç­–ææ¡ˆ {'='*20}")
    print("1. ğŸ“ **ãƒ‡ãƒ¼ã‚¿é©å¿œå‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: ã‚«ãƒ†ã‚´ãƒªæ¤œå‡ºã«ã‚ˆã‚‹è‡ªå‹•èª¿æ•´")
    print("2. ğŸ¨ **ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæœ€é©åŒ–**: èªå½™æ•°ã«å¿œã˜ãŸç”»åƒã‚µã‚¤ã‚ºãƒ»å¯†åº¦èª¿æ•´") 
    print("3. ğŸ”„ **å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: èªå½™å¯†åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘èª¿æ•´")
    print("4. ğŸ“± **ãƒãƒ«ãƒãƒ—ãƒªã‚»ãƒƒãƒˆ**: ç”¨é€”åˆ¥ã®æœ€é©åŒ–æ¸ˆã¿è¨­å®š")
    
    # JSONå‡ºåŠ›
    output_data = {
        'analysis_results': analysis_results,
        'optimization_strategies': strategies
    }
    
    with open('outputs/adaptive_analysis_results.json', 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ åˆ†æçµæœä¿å­˜: outputs/adaptive_analysis_results.json")

if __name__ == "__main__":
    main()