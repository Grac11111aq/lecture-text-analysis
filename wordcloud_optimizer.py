#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ•™è‚²ãƒ‡ãƒ¼ã‚¿ã®èªå½™ã‚’é«˜åº¦ã«å‡¦ç†ã—ã¦è³ªã®é«˜ã„ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆ
"""

import pandas as pd
import re
from collections import Counter
from janome.tokenizer import Tokenizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import font_manager

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAGothic', 'VL Gothic', 'Noto Sans CJK JP']

class WordCloudOptimizer:
    def __init__(self):
        self.tokenizer = Tokenizer()
        
        # é«˜é »åº¦ãƒã‚¤ã‚ºèªï¼ˆåˆ†æçµæœã«åŸºã¥ãï¼‰
        self.noise_words = {
            'ã‹ã‚‰', 'ã§ã™', 'ã‚ã‚ŠãŒã¨ã†', 'æ±äº¬é«˜å°‚', 'ã¸', 'ã¿ãªã•ã‚“', 
            'ä»Šæ—¥', 'ã‚ˆã‚Š', 'ã“ã¨', 'ä¸€ç•ª', 'ã¾ã™', 'ã§ã—', 'ã¾ã—',
            'ãŒ', 'ã¦', 'ã„ã‚‹', 'ãŸ', 'ã®', 'ã«', 'ã¯', 'ã‚’', 'ã§',
            'å…¥ã£', 'ã—', 'ãªã£', 'ã£', 'ã„', 'ã‚“', 'ã‚‹', 'ã‚Œ'
        }
        
        # ç§‘å­¦ç”¨èªï¼ˆå„ªå…ˆè¡¨ç¤ºãƒ»é‡ã¿å¢—åŠ ï¼‰
        self.science_terms = {
            'ãƒŠãƒˆãƒªã‚¦ãƒ ': 3.0,
            'å¡©': 2.5, 
            'é£Ÿå¡©': 2.5,
            'å¡©åˆ†': 2.5,
            'çµæ™¶': 2.8,
            'å®Ÿé¨“': 2.8,
            'ç‚è‰²åå¿œ': 3.0,
            'å†çµæ™¶': 3.0,
            'æˆåˆ†': 2.0,
            'å¡©åŒ–ãƒŠãƒˆãƒªã‚¦ãƒ ': 3.0,
            'Na': 3.0,
            'NaCl': 3.0
        }
        
        # æ„Ÿæƒ…ãƒ»è©•ä¾¡èªï¼ˆæ•™è‚²åŠ¹æœè¡¨ç¾ï¼‰
        self.emotion_terms = {
            'å°è±¡': 2.0,
            'ãã‚Œã„': 2.0,
            'é¢ç™½ã„': 2.2,
            'ãŠã‚‚ã—ã‚ã„': 2.2,
            'æ¥½ã—ã„': 2.2,
            'å¥½ã': 2.0,
            'èˆˆå‘³': 2.5,
            'ã³ã£ãã‚Š': 2.0,
            'ã™ã”ã„': 2.0
        }
        
        # è¡¨è¨˜ã‚†ã‚Œçµ±ä¸€ãƒ«ãƒ¼ãƒ«
        self.normalization_rules = {
            # ã¿ãç³»çµ±ä¸€
            'ã¿ãæ±': 'ã¿ã',
            'ã¿ãã—ã‚‹': 'ã¿ã',
            'å‘³å™Œ': 'ã¿ã',
            'å‘³å™Œæ±': 'ã¿ã',
            
            # é¢ç™½ã„ç³»çµ±ä¸€
            'ãŠã‚‚ã—ã‚ã„': 'é¢ç™½ã„',
            'ãŠã‚‚ã—ã‚ã': 'é¢ç™½ã„',
            'ãŠã‚‚ã—ã‚ã‹ã£': 'é¢ç™½ã„',
            
            # æ¥½ã—ã„ç³»çµ±ä¸€
            'ãŸã®ã—ã„': 'æ¥½ã—ã„',
            'ãŸã®ã—ã': 'æ¥½ã—ã„',
            'ãŸã®ã—ã‹ã£': 'æ¥½ã—ã„',
            
            # å¡©ç³»çµ±ä¸€ï¼ˆæœ€é‡è¦ï¼‰
            'ãˆã‚“åˆ†': 'å¡©åˆ†',
            'ã‚¨ãƒ³åˆ†': 'å¡©åˆ†',
            
            # æº¶ã‘ã‚‹ç³»çµ±ä¸€
            'ã¨ã‘ã‚‹': 'æº¶ã‘ã‚‹',
            'ã¨ã‘': 'æº¶ã‘ã‚‹',
            'ã¨ã‹ã—': 'æº¶ã‘ã‚‹',
            'æº¶ã‹ã—': 'æº¶ã‘ã‚‹'
        }
        
        # æœ€å°èªé•·ï¼ˆ1æ–‡å­—èªé™¤å¤–ï¼‰
        self.min_word_length = 2
        
    def preprocess_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
        text = str(text)
        # æ•°å­—ãƒ»è¨˜å·é™¤å»
        text = re.sub(r'[0-9ï¼-ï¼™]', '', text)
        text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿãƒ»ã€Œã€ã€ã€ï¼ˆï¼‰()]', '', text)
        return text
    
    def extract_and_filter_words(self, df):
        """èªå½™æŠ½å‡ºãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»æ­£è¦åŒ–"""
        filtered_words = []
        
        for idx, row in df.iterrows():
            text = self.preprocess_text(row['text'])
            category = row['category']
            
            # å½¢æ…‹ç´ è§£æ
            tokens = self.tokenizer.tokenize(text)
            for token in tokens:
                try:
                    word = token.surface
                    
                    # åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if (len(word) >= self.min_word_length and 
                        word not in self.noise_words and
                        not re.match(r'^[ã-ã‚“]{1}$', word)):  # ã²ã‚‰ãŒãª1æ–‡å­—é™¤å¤–
                        
                        # è¡¨è¨˜ã‚†ã‚Œçµ±ä¸€
                        if word in self.normalization_rules:
                            word = self.normalization_rules[word]
                        
                        filtered_words.append((word, category))
                        
                except (AttributeError, IndexError):
                    continue
        
        return filtered_words
    
    def calculate_word_weights(self, words):
        """èªå½™é‡ã¿è¨ˆç®—ï¼ˆæ•™è‚²çš„ä¾¡å€¤åæ˜ ï¼‰"""
        word_freq = Counter([word for word, category in words])
        weighted_freq = {}
        
        for word, freq in word_freq.items():
            base_weight = freq
            
            # ç§‘å­¦ç”¨èªé‡ã¿å¢—åŠ 
            if word in self.science_terms:
                weight = base_weight * self.science_terms[word]
            # æ„Ÿæƒ…èªé‡ã¿å¢—åŠ 
            elif word in self.emotion_terms:
                weight = base_weight * self.emotion_terms[word]
            else:
                weight = base_weight
            
            weighted_freq[word] = weight
        
        return weighted_freq
    
    def generate_optimized_wordcloud(self, df, title="æœ€é©åŒ–ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", 
                                   width=1200, height=800):
        """æœ€é©åŒ–ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ"""
        # èªå½™å‡¦ç†
        words = self.extract_and_filter_words(df)
        weighted_freq = self.calculate_word_weights(words)
        
        # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        font_path = 'fonts/ipaexg.ttf'
        
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰è¨­å®š
        wordcloud = WordCloud(
            font_path=font_path,
            width=width,
            height=height,
            background_color='#f8f8f8',
            max_words=120,
            min_font_size=20,
            max_font_size=180,
            prefer_horizontal=0.9,
            relative_scaling=0.3,
            colormap='plasma'  # æ—¢å­˜ã®ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚’ä½¿ç”¨
        )
        
        # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ
        if weighted_freq:
            wordcloud.generate_from_frequencies(weighted_freq)
            
            # å¯è¦–åŒ–
            plt.figure(figsize=(15, 10))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title(title, fontsize=20, fontweight='bold', pad=20)
            plt.tight_layout()
            
            return wordcloud, weighted_freq
        else:
            print("âš ï¸ æœ‰åŠ¹ãªèªå½™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None, {}
    
    def compare_before_after(self, df):
        """æ”¹å–„å‰å¾Œã®æ¯”è¼ƒåˆ†æ"""
        print("ğŸ” **æ”¹å–„å‰å¾Œæ¯”è¼ƒåˆ†æ**")
        print("=" * 50)
        
        # åŸæ–‡åˆ†æ
        original_words = []
        for text in df['text']:
            tokens = self.tokenizer.tokenize(str(text))
            for token in tokens:
                try:
                    word = token.surface
                    if len(word) >= 1:
                        original_words.append(word)
                except:
                    continue
        
        # æœ€é©åŒ–å¾Œåˆ†æ
        optimized_words = self.extract_and_filter_words(df)
        optimized_word_list = [word for word, category in optimized_words]
        
        original_freq = Counter(original_words)
        optimized_freq = Counter(optimized_word_list)
        
        print(f"ğŸ“Š èªå½™æ•°å¤‰åŒ–:")
        print(f"  åŸæ–‡: {len(original_freq)}èª (ç·{len(original_words)}èª)")
        print(f"  æœ€é©åŒ–å¾Œ: {len(optimized_freq)}èª (ç·{len(optimized_word_list)}èª)")
        print(f"  ãƒã‚¤ã‚ºé™¤å»ç‡: {(1 - len(optimized_word_list)/len(original_words))*100:.1f}%")
        
        print(f"\nğŸ¯ ç§‘å­¦ç”¨èªé »åº¦å¤‰åŒ–:")
        for term in ['ãƒŠãƒˆãƒªã‚¦ãƒ ', 'å¡©', 'çµæ™¶', 'å®Ÿé¨“']:
            orig_count = original_freq.get(term, 0)
            opt_count = optimized_freq.get(term, 0)
            print(f"  {term}: {orig_count} â†’ {opt_count}å›")
        
        print(f"\nğŸ—‘ï¸ é™¤å»ã•ã‚ŒãŸãƒã‚¤ã‚ºèªï¼ˆä¸Šä½10èªï¼‰:")
        removed_words = set(original_freq.keys()) - set(optimized_freq.keys())
        removed_freq = {word: original_freq[word] for word in removed_words}
        for word, freq in sorted(removed_freq.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {word}: {freq}å›")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ¨ **ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æœ€é©åŒ–å®Ÿè¡Œ**")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = pd.read_csv('data/processed/all_text_corpus.csv')
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿: {len(df)}ä»¶")
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
    optimizer = WordCloudOptimizer()
    
    # æ¯”è¼ƒåˆ†æ
    optimizer.compare_before_after(df)
    
    # æœ€é©åŒ–ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ
    print(f"\nğŸ¨ æœ€é©åŒ–ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆä¸­...")
    wordcloud, weighted_freq = optimizer.generate_optimized_wordcloud(
        df, title="æœ€é©åŒ–ã•ã‚ŒãŸãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆæ•™è‚²åŠ¹æœé‡è¦–ï¼‰"
    )
    
    if wordcloud:
        # ä¿å­˜
        output_path = 'outputs/wordclouds/optimized_wordcloud.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_path}")
        
        # é‡è¦èªå½™è¡¨ç¤º
        print(f"\nâ­ **é‡è¦èªå½™ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé‡ã¿ä»˜ãï¼‰**")
        for word, weight in sorted(weighted_freq.items(), key=lambda x: x[1], reverse=True)[:15]:
            print(f"  {word}: {weight:.1f}")
        
        plt.show()
    
    print("\nâœ… **æœ€é©åŒ–å®Œäº†**")

if __name__ == "__main__":
    main()